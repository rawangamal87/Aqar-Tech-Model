# -*- coding: utf-8 -*-
"""AQAR TECH.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HVETluid2NtZ_BycbFCZ63SNnanYkygW
"""

import json
import pandas as pd
import numpy as np
import spacy
import matplotlib.pyplot as plt
import seaborn as sns
import requests
from geopy.distance import geodesic
import ast


# For classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix

import re
import spacy
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.utils import resample

# For deployment(Saving model)
import pickle
import joblib

url = "https://demo.aqar-tech.com/api/v1/messages"

all_messages = []

for page in range(1, 56):
    params = {"page": page}
    response = requests.get(url, params=params)

    if response.status_code == 200:
        data = response.json()
        messages = data["result"]["data"]
        all_messages.extend(messages)
        print(f"Loaded page {page} with {len(messages)} messages.")
    else:
        print(f"Failed to load page {page}, status: {response.status_code}")

df = pd.DataFrame(all_messages)
print(f"\nTotal messages collected: {len(df)}")
print(df)
len(df)

print(df.columns)

df['address'].iloc[244]

df['region'].iloc[244]

df.tail()

df['message'].isnull().sum()

df.rename(columns={'message_type_ar': 'label'}, inplace=True)
df

df['label'].value_counts()

cleaned_df = df.dropna(subset='label')

cleaned_df['label'].isnull().sum()

cleaned_df = ['id', 'message','cleaned_message_for_model', 'label', 'city_name', 'region_name', 'price', 'area', 'address']
cleaned_df

"""# ***Cleaning***

remove emojis
"""

def remove_emojis(text):
    if not isinstance(text, str): return text
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "\U00002500-\U00002BEF"
        "\U00002702-\U000027B0"
        "\U0001F900-\U0001F9FF"
        "\U0001FA70-\U0001FAFF"
        "\u2600-\u26FF"
        "\u2700-\u27BF"
        "\uFE0F"
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.sub('', text)

def remove_diacritics(text):
    if not isinstance(text, str): return text
    return re.sub(r'[\u064B-\u0652\u0670\u0640]', '', text)

def fix_repeated_chars(text):
    if not isinstance(text, str): return text
    return re.sub(r'(.)\1{2,}', r'\1', text)

def clean_text(text):
    if not isinstance(text, str): return text
    text = re.sub(r'<.*?>', ' ', text)
    text = re.sub(r'[^\w\s\u0600-\u06FF]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_city_name(city_cell):
    try:
        if isinstance(city_cell, str): city_cell = ast.literal_eval(city_cell)
        if isinstance(city_cell, dict):
            translations = city_cell.get('translations', [])
            for item in translations:
                if item.get('locale') == 'ar':
                    return item.get('name')
            return city_cell.get('name')
    except:
        return None

def extract_region_name(city_cell):
    try:
        if isinstance(city_cell, str): city_cell = ast.literal_eval(city_cell)
        if isinstance(city_cell, dict):
            translations = city_cell.get('translations', [])
            for item in translations:
                if item.get('locale') == 'ar':
                    return item.get('name')
            return city_cell.get('name')
    except:
        return None

# نسخ DataFrame لتجنب التحذيرات
cleaned_df = df.copy()

# 1. تنظيف عمود الرسائل بشكل متسلسل
def combined_clean(text):
    text = remove_emojis(text)
    text = remove_diacritics(text)
    text = fix_repeated_chars(text)
    text = clean_text(text)
    return text

# تغيير اسم العمود هنا إلى 'cleaned_for_model'
cleaned_df['cleaned_message_for_model'] = cleaned_df['message'].apply(combined_clean)

# 2. استخراج أسماء المدن والمناطق
cleaned_df['city_name'] = cleaned_df['city'].apply(extract_city_name)
cleaned_df['region_name'] = cleaned_df['region'].apply(extract_region_name)

# 3. توحيد وتصحيح عمود المساحة
cleaned_df['area'] = cleaned_df['area'].astype(str).str.replace(',', '').str.strip()
cleaned_df['area'] = pd.to_numeric(cleaned_df['area'], errors='coerce')

# 4. حذف الصفوف الفارغة من الأعمدة المهمة
required_columns = ['id', 'cleaned_message_for_model', 'city_name', 'region_name', 'price', 'area', 'address']
cleaned_df = cleaned_df.dropna(subset=required_columns)

print(cleaned_df[['cleaned_message_for_model', 'area', 'city_name', 'region_name', ]])

cleaned_df

"""extract label based on message"""

def extract_label(message):
    if pd.isna(message):
        return None
    text = message.strip().casefold()

    if re.search(r'\b(للبيع|متوفر|للايجار|عرض|يوجد)\b', text):
        return 'عرض'
    elif re.search(r'\b(مطلوب|شراء|اريد|احتاج|أحتاج|نشتري|شراء)\b', text):
        return 'طلب'
    else:
        return None

cleaned_df.loc[:, 'label'] = cleaned_df['cleaned_message_for_model'].apply(extract_label)

"""Extract Area

Extract City
"""

cleaned_df

def clean_real_estate_message_v3(text):
    if not isinstance(text, str):
        return text

    # 1. إزالة HTML tags مثل <br />
    text = re.sub(r'<[^>]+>', ' ', text)

    # 2. إزالة روابط الإنترنت
    text = re.sub(r'https?://\S+', '', text)

    # 3. إزالة المساحة مثل: "المساحة 500" أو "مساحه: 450"
    text = re.sub(r'المساح[ةه]\s*\d+[.,]?\d*', '', text)

    # 4. إزالة الأطوال مثل: "الاطوال 20&25"
    text = re.sub(r'الاطوال\s*\d+\s*&\s*\d+', '', text)

    # 5. إزالة الواجهة: "الواجهة غربية"
    text = re.sub(r'الواجه[ةه]\s*(شمالية|جنوبية|شرقية|غربية)', '', text)

    # 6. إزالة عرض الشارع مثل: "عرض الشارع 20"
    text = re.sub(r'عرض\s+الشارع\s*\d+[.,]?\d*', '', text)

    # 7. إزالة السعر للمتر: "البيع ٤٨٠٠ لمتر" أو "السعر للمتر"
    text = re.sub(r'البيع\s*\d+[.,]?\d*\s*ل?متر', '', text)

    # 8. إزالة فواصل ومسافات زائدة
    text = re.sub(r'\s+', ' ', text).strip()

    return text

final_columns = ['id', 'cleaned_message_for_model', 'label', 'city_name', 'region_name', 'price', 'area', 'address']
cleaned_df = cleaned_df[final_columns].dropna()
cleaned_df

Q1 = cleaned_df['area'].quantile(0.25)
Q3 = cleaned_df['area'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

cleaned_df = cleaned_df[(cleaned_df['area'] >= lower_bound) & (cleaned_df['area'] <= upper_bound)]

cleaned_df['region_name'].value_counts()

cleaned_df.columns

"""# ***Splitting Data***"""

x = cleaned_df['cleaned_message_for_model']
y_label = cleaned_df['label']
y_area = cleaned_df['area']
y_price = cleaned_df['price']
y_city_name = cleaned_df['city_name']
x_train, x_test, y_label_train, y_label_test, y_area_train, y_area_test, y_price_train, y_price_test, y_city_train, y_city_test = train_test_split(x, y_label, y_area, y_price, y_city_name, test_size=0.2, random_state=42)

vectorizer = TfidfVectorizer(ngram_range= (1,2),max_features=700)
x_train_tfidf = vectorizer.fit_transform(x_train)
x_test_tfidf = vectorizer.transform(x_test)

"""# ***Modeling***


1.   Label
2.   Area
3.   Price
2.   City

**Modeling for Label**
"""

label_encoder = LabelEncoder()
y_label_train_encoded = label_encoder.fit_transform(y_label_train)
y_label_test_encoded = label_encoder.transform(y_label_test)

model_label = XGBClassifier()
model_label.fit(x_train_tfidf, y_label_train_encoded)

model_label.score(x_train_tfidf, y_label_train_encoded)

model_label.score(x_test_tfidf, y_label_test_encoded)

"""**modeling for area**"""

y_area_train = pd.to_numeric(y_area_train, errors='coerce')
y_area_test = pd.to_numeric(y_area_test, errors='coerce')

y_area_train = y_area_train.fillna(0)
y_area_test = y_area_test.fillna(0)

from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()
y_area_train_scaled = min_max_scaler.fit_transform(y_area_train.values.reshape(-1, 1)).ravel()
y_area_train_scaled
y_area_test_scaled = min_max_scaler.transform(y_area_test.values.reshape(-1, 1)).ravel()
y_area_test_scaled

from sklearn.ensemble import RandomForestRegressor
rfr = RandomForestRegressor(n_estimators=250)
model_area = rfr.fit(x_train_tfidf, y_area_train_scaled)

model_area.score(x_train_tfidf, y_area_train_scaled)

model_area.score(x_test_tfidf, y_area_test_scaled)

predicted_area_scaled = model_area.predict(x_train_tfidf)
predicted_area = min_max_scaler.inverse_transform(predicted_area_scaled.reshape(-1, 1)).ravel()

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score

max_depth_values = [5, 10, 15, 20, 25, None]

for depth in max_depth_values:
    rfr = RandomForestRegressor(n_estimators=100, max_depth=depth, random_state=42)

    rfr.fit(x_train_tfidf, y_area_train_scaled)

    train_score = r2_score(y_area_train_scaled, rfr.predict(x_train_tfidf))
    test_score = r2_score(y_area_test_scaled, rfr.predict(x_test_tfidf))

    print(f"Max_depth: {depth}")
    print(f"  Training Score: {train_score:.4f}")
    print(f"  Testing Score:  {test_score:.4f}")
    print("-" * 30)

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score

gbr_area = GradientBoostingRegressor(n_estimators=250, learning_rate=0.2, max_depth=4, random_state=42)

gbr_area.fit(x_train_tfidf, y_area_train_scaled)

train_pred_area = gbr_area.predict(x_train_tfidf)
test_pred_area = gbr_area.predict(x_test_tfidf)

train_score_area = r2_score(y_area_train_scaled, train_pred_area)
test_score_area = r2_score(y_area_test_scaled, test_pred_area)

print(f"Gradient Boosting Training Score (Predicting Area): {train_score_area:.4f}")
print(f"Gradient Boosting Testing Score (Predicting Area): {test_score_area:.4f}")

"""**Modeling for city**"""

from xgboost import XGBClassifier

city_encoder = LabelEncoder()
y_city_train_encoded = city_encoder.fit_transform(y_city_train)
y_city_test_encoded = city_encoder.transform(y_city_test)

model_city = XGBClassifier()
model_city.fit(x_train_tfidf, y_city_train_encoded)

model_city.score(x_train_tfidf, y_city_train_encoded)

model_city.score(x_test_tfidf, y_city_test_encoded)

"""**Modeling for price**"""

from sklearn.ensemble import RandomForestRegressor
RandomForestRegressor = RandomForestRegressor(n_estimators=250)
model_price = RandomForestRegressor.fit(x_train_tfidf, y_price_train)

model_price.score(x_train_tfidf, y_price_train)

model_price.score(x_test_tfidf, y_price_test)

import pandas as pd
import numpy as np

# تحويل الأسعار لأرقام
y_price_train = pd.to_numeric(y_price_train, errors='coerce')
y_price_test = pd.to_numeric(y_price_test, errors='coerce')

# إنشاء الماسك
mask_train = ~y_price_train.isna()
mask_test = ~y_price_test.isna()

# تحويل الماسك لقائمة فهارس
idx_train = np.where(mask_train)[0]
idx_test = np.where(mask_test)[0]

# فلترة البيانات
x_train_tfidf = x_train_tfidf[idx_train]
x_test_tfidf = x_test_tfidf[idx_test]
y_price_train = y_price_train.iloc[idx_train]
y_price_test = y_price_test.iloc[idx_test]

# Log transform
y_price_train_log = np.log1p(y_price_train)
y_price_test_log = np.log1p(y_price_test)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
y_price_train_log_scaled = sc.fit_transform(y_price_train_log.values.reshape(-1, 1)).ravel()
y_price_test_log_scaled = sc.transform(y_price_test_log.values.reshape(-1, 1)).ravel()

from sklearn.ensemble import RandomForestRegressor
RandomForestRegressor = RandomForestRegressor(n_estimators=250)
model_price = RandomForestRegressor.fit(x_train_tfidf, y_price_train_log_scaled)

model_price.score(x_train_tfidf, y_price_train_log_scaled)

model_price.score(x_test_tfidf, y_price_test_log_scaled)

"""**Saving models**"""

joblib.dump(model_label,'model_label.pkl')

joblib.dump(model_area,'model_area.pkl')

joblib.dump(model_city,'model_city.pkl')

joblib.dump(vectorizer,'vectorizer.pkl')

joblib.dump(label_encoder,'label_encoder.pkl')

joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

joblib.dump(min_max_scaler,'min_max_scaler.pkl')

joblib.dump(city_encoder,'city_encoder.pkl')

joblib.dump(sc,'sc.pkl')

joblib.dump(model_price,'model_price.pkl')