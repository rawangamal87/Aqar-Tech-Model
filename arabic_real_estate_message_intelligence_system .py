# -*- coding: utf-8 -*-
"""Arabic Real Estate Message Intelligence System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tT62gx8DjlTcT5q53C3BiT8JOaaRJhIA

# ***Welcome Real Estate project for Aqar Tech Company***
This notebook presents a real NLP-based solution to process real estate messages received via WhatsApp.  
The main objectives:
- Extract key information
- Filter sensitive content (phone numbers, prices)
- Prepare the data for a dashboard view

## Special Thanks
Special thanks to **Aqar Tech** for providing the opportunity to work on this task.  
I'm excited to contribute and showcase my ability to deliver a clean, well-structured NLP pipeline.

Prepared by: **Rawan Gamal |  AI & NLP Engineer**  
Date: July 2025

*   Connect with me on LinkedIn:(https://linkedin.com/in/rawan-gamal-41aa0024b)

# ***Tools & Libraries used***
"""

import json
import pandas as pd
import numpy as np
import spacy
import matplotlib.pyplot as plt
import seaborn as sns
import requests
from geopy.distance import geodesic
import ast
import emoji


# For classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix

import re
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.utils import resample

# For deployment(Saving model)
import pickle
import joblib

pip install emoji

"""# ***Load & Read Json File***"""

url = "https://demo.aqar-tech.com/api/v1/messages"

all_messages = []   # The first 10 pages

for page in range(1, 11):
    params = {"page": page}
    response = requests.get(url, params=params)

    if response.status_code == 200:
        data = response.json()
        messages = data["result"]["data"]
        all_messages.extend(messages)
        print(f"Loaded page {page} with {len(messages)} messages.")
    else:
        print(f"Failed to load page {page}, status: {response.status_code}")

df = pd.DataFrame(all_messages)
print(f"\nTotal messages collected: {len(df)}")
print(df)

df.tail(25)

# num of rows
print(f"Total messages collected: {len(df)}")

df.info()

df.columns

len(df.columns)

"""# ***Text processing***"""

print(df['message'].iloc[259])

df['message'].fillna('', inplace=True)
display(df[['message']].isnull().sum())      #num of nan

# message_type_ar	>>> label
df.rename(columns={'message_type_ar': 'label'}, inplace=True)
df

df = df.dropna(subset=['label'])
df

df.isnull().sum()

# Cleaning rows from NaN values
df = df[df['city'].notnull()]
df = df[df['state'].notnull()]
df = df[df['area'].notnull()]
df = df[df['id'].notnull()]

df = df[df['label'] != 'none']

# Selection for important rows and add them in new DF >>>>> cleaned_df
cleaned_df= df[['id','message', 'label', 'city','state', 'area']]
cleaned_df.tail(20)

print(cleaned_df['label'].value_counts())

# Classify message(Feature) to extract طلب ولا عرض and add it in label

def extract_label(message):
    if pd.isna(message):
        return None
    text = message.strip().casefold()

    if re.search(r'\b(للبيع|عرض|يوجد)\b', text):
        return 'عرض'
    elif re.search(r'\b(مطلوب|نشتري|شراء)\b', text):
        return 'طلب'
    else:
        return None

cleaned_df.loc[:, 'label'] = cleaned_df['message'].apply(extract_label)

print(cleaned_df['label'].value_counts())

"""remove emojis"""

print(cleaned_df.head())

print(cleaned_df.columns)

"""# ***Full Cleaning for text***








"""

import emoji

def convert_arabic_nums(text):
    return text.translate(str.maketrans('٠١٢٣٤٥٦٧٨٩', '0123456789'))

def remove_emojis(text):
    return emoji.replace_emoji(text, replace='')

def clean_message_for_model(message):
    if not isinstance(message, str):
        return ""

    message = convert_arabic_nums(message)
    message = message.replace("�", "")
    message = remove_emojis(message)

    message = re.sub(r'<br\s*/?>', ' ', message, flags=re.IGNORECASE)
    message = re.sub(r'<.*?>', ' ', message)

    message = re.sub(r'(?:\+966|00966|0)?5\d{8}', '[]', message)
    message = re.sub(r'[\u064B-\u0652]', '', message)
    message = re.sub(r'(.)\1{2,}', r'\1', message)

    message = re.sub(
        r'(\d{1,3}(?:[.,]?\d{3})+|\d+)\s*(ريال|ر\.س|جنيه|ج|دولار|ألف|الف|مليون|ريالات|م²|متر)?',
        '[]',
        message
    )

    message = re.sub(r'[^\w\s]', '', message)
    message = re.sub(r'\s+', ' ', message).strip()

    return message

cleaned_df['cleaned_message_for_model'] = cleaned_df['message'].apply(clean_message_for_model)

cleaned_df

"""# ***Data Extraction***"""

def convert_arabic_nums(text):
    return text.translate(str.maketrans('٠١٢٣٤٥٦٧٨٩', '0123456789'))

def remove_emojis(text):
    emoji_pattern = re.compile(
        r'[^\w\s\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF]',
        flags=re.UNICODE
    )
    return emoji_pattern.sub('', text)

def clean_message_for_model(message):
    if not isinstance(message, str):
        return ""

    message = convert_arabic_nums(message)
    message = message.replace("�", "")
    message = remove_emojis(message)

    message = re.sub(r'<br\s*/?>', ' ', message, flags=re.IGNORECASE)
    message = re.sub(r'<.*?>', ' ', message)

    message = re.sub(r'(?:\+966|00966|0)?5\d{8}', '[PHONE]', message)

    message = re.sub(r'[\u064B-\u0652]', '', message)
    message = re.sub(r'(.)\1{2,}', r'\1', message)

    message = re.sub(
        r'(\d{1,3}(?:[.,]?\d{3})+|\d+)\s*(ريال|ر\.س|جنيه|ج|دولار|ألف|الف|مليون|ريالات|م²|متر)?',
        '[PRICE]',
        message
    )

    message = re.sub(r'[^\w\s]', '', message)
    message = re.sub(r'\s+', ' ', message).strip()

    return message

cleaned_df['cleaned_message_for_model'] = cleaned_df['message'].apply(clean_message_for_model)

def links(text):
    if not isinstance(text, str):
        return None

    pattern = r'(https?://\S+|www\.\S+|\S+\.(com|net|org|ly|link|gl|app)\S*)'
    matches = re.findall(pattern, text)
    links = [match[0] if isinstance(match, tuple) else match for match in matches]

    return links if links else None

def remove_links(text):
    if not isinstance(text, str):
        return text

    pattern = r'(https?://(?:www\.)?(?:goo\.gl/maps|maps\.app\.goo\.gl|maps\.google\.com)/\S+|https?maps?app?goo?gl\S+|wa\.link/\S+|\S+\.(com|net|org|ly|link|gl|app)\S*)'
    return re.sub(pattern, '', text).strip()

cleaned_df['links'] = cleaned_df['message'].apply(links)
cleaned_df['cleaned_message_for_model'] = cleaned_df['message'].apply(remove_links)

cleaned_df['links'] = cleaned_df['message'].apply(links)

cleaned_df['links'] = cleaned_df['links'].apply(lambda x: 'no link' if x is None else x)

#City
def extract_city_name(city_cell):
    try:
        if isinstance(city_cell, str):
            city_cell = ast.literal_eval(city_cell)

        if isinstance(city_cell, dict):
            translations = city_cell.get('translations', [])
            for item in translations:
                if item.get('locale') == 'ar':
                    return item.get('name')

            return city_cell.get('name')
    except:
        return None

cleaned_df['city_name'] = cleaned_df['city'].apply(extract_city_name)

cleaned_df

cleaned_df['cleaned_message_for_model'].iloc[157]

# Oversampling >>>>> to handle unbalance data

df_demand = cleaned_df[cleaned_df['label'] == 'طلب']
df_offer = cleaned_df[cleaned_df['label'] == 'عرض']

if len(df_demand) == 0:
    print("Warning: No 'طلب' messages found for upsampling.")
    df_balanced = df_offer.copy()
elif len(df_offer) == 0:
    print("Warning: No 'عرض' messages found for upsampling.")
    df_balanced = df_demand.copy()
else:
    df_demand_upsampled = resample(df_demand, replace= True, n_samples= len(df_offer), random_state= 42)
    df_balanced = pd.concat([df_offer, df_demand_upsampled])
    df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)

texts = df_balanced['cleaned_message_for_model']
labels = df_balanced['label']

df_balanced

print(texts)

le = LabelEncoder()
y = le.fit_transform(df_balanced['label'])
print(y)
print(le.classes_)

df_balanced.head(10)

"""# ***Modeling***
Machine Learning
Deep Learning & LSTM
"""

print(len(df_balanced))
print(df_balanced['cleaned_message_for_model'].isnull().sum())

df_balanced = df_balanced[df_balanced['label'].notnull()]
print(len(df_balanced))

cleaned_df['label'] = cleaned_df['message'].apply(extract_label)

#TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X = vectorizer.fit_transform(texts)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=le.classes_))
print(confusion_matrix(y_test, y_pred))

model.score(X_train, y_train)

model.score(X_test, y_test)

joblib.dump(vectorizer, "tfidf_vectorizer.pkl")

joblib.dump(model, "xgb_model.pkl")

joblib.dump(le, "label_encoder.pkl")

model.save_model("model.json")